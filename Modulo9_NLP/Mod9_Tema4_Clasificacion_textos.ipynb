{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características textuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # dimensiones de la matriz: cantidad de documento x características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de las características\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray() # matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de las características e índice de la columna se almacena en el atributo vocabulary_ del vectorizador\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "# vectorizer.vocabulary_.get('document') # acceso al índice de la columna del token específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# las palabras que no se vieron en el corpus de entrenamiento \n",
    "# se ignorarán por completo en futuras llamadas al método de transformación\n",
    "\n",
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['documento', 'segundo', 'tercer'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    'Esto es un documento',\n",
    "    'Este es el segundo documento',\n",
    "    'Y el tercer documento'\n",
    "]\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('spanish')\n",
    "\n",
    "vec = CountVectorizer(stop_words=stop)\n",
    "X = vec.fit_transform(docs).toarray()\n",
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 21) \n",
      "\n",
      "{'this': 18, 'is': 5, 'the': 12, 'first': 3, 'document': 2, 'this is': 19, 'is the': 6, 'the first': 13, 'first document': 4, 'second': 9, 'the second': 14, 'second second': 11, 'second document': 10, 'and': 0, 'third': 16, 'one': 8, 'and the': 1, 'the third': 15, 'third one': 17, 'is this': 7, 'this the': 20} \n",
      "\n",
      "[[0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0]\n",
      " [0 0 1 0 0 1 1 0 0 2 1 1 1 0 1 0 0 0 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "print(X_2.shape, '\\n')\n",
    "\n",
    "print(bigram_vectorizer.vocabulary_, '\\n')\n",
    "\n",
    "print(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'and the', 'document', 'first', 'first document', 'is',\n",
       "       'is the', 'is this', 'one', 'second', 'second document',\n",
       "       'second second', 'the', 'the first', 'the second', 'the third',\n",
       "       'third', 'third one', 'this', 'this is', 'this the'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "X_2[:, feature_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf=tfidf_vect.fit_transform(corpus)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names_out())\n",
    "print(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de textos\n",
    "\n",
    "En esta clase, exploraremos datos de mensajes de texto y crearemos modelos para predecir si un mensaje es spam o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "#conjunto de entrenamiento\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide en conjunto de train y test donde el target es si el correo es spam o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dividir el corpus en un conjunto de entrenamiento 75% y uno de prueba 25% por default \n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'],\n",
    "                                                    spam_data['target'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872                       I'll text you when I drop x off\n",
       "831     Hi mate its RV did u hav a nice hol just a mes...\n",
       "1273    network operator. The service is free. For T &...\n",
       "3314    FREE MESSAGE Activate your 500 FREE Text Messa...\n",
       "4929    Hi, the SEXYCHAT girls are waiting for you to ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872     0\n",
       "831     0\n",
       "1273    1\n",
       "3314    1\n",
       "4929    1\n",
       "Name: target, dtype: int32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué porcentaje de los documentos en `spam_data` son spam?\n",
    "\n",
    "*Esta función debe devolver un valor flotante, el valor porcentual (es decir, $ ratio * 100 $).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_uno():\n",
    "    # spam_data['target'].mean() # extrae la media\n",
    "    spam=spam_data[spam_data['target'] != 0]\n",
    "    print(len(spam), len(spam_data), len(spam)*100/len(spam_data))\n",
    "    return spam_data['target'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 5572 13.406317300789663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_uno()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados. Ajuste un modelo de clasificación Naive Bayes multinomial. \n",
    "\n",
    "Transforme otro modelo de clasificación pero ajustando los datos de entrenamiento: eliminando las palabras vacías o cerradas, y añadiéndole el conteo de bigramas de palabras.\n",
    "\n",
    "Devuelva las métricas de evaluación de los modelos predictivos resultantes usando los datos de prueba transformados: área bajo la curva (AUC), accuracy y F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score #son metricas - area debajo de la curva roc\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo de uso de  CountVectorizer()  [Modelo de formación de la matriz termino-documento ]+ MultinomialNB()  [Modelo de aprendizaje supervisado de clasificación]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones y resultados del clasificador 1\n",
      "Predicciones [0 0 0 ... 0 0 1]\n",
      "Accuracy 0.9863603732950467\n",
      "AUC 0.9581366823421557\n",
      "F1 0.9501312335958005\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "clf1 = MultinomialNB()\n",
    "clf1.fit(X_train_vectorized, y_train) # ajusta al calificador 1\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "predictions1 = clf1.predict(X_test_vectorized) # funcion para predecir 1\n",
    "\n",
    "print('Predicciones y resultados del clasificador 1')\n",
    "print('Predicciones', predictions1)\n",
    "# métricas para verificar que tanto es el calificador es correcto\n",
    "print( 'Accuracy', accuracy_score(y_test, predictions1) )\n",
    "print( 'AUC', roc_auc_score(y_test, predictions1) )\n",
    "print( 'F1', f1_score(y_test, predictions1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo de uso de  CountVectorizer()  [Modelo de formación de la matriz termino-documento ]+ MultinomialNB()  [Modelo de aprendizaje supervisado de clasificación] AGREGANDO stop_words=stop, ngram_range=(1, 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones y resultados del clasificador 2\n",
      "Predicciones [0 0 0 ... 0 0 1]\n",
      "Accuracy 0.9892318736539842\n",
      "AUC 0.9640489448754732\n",
      "F1 0.9606299212598426\n"
     ]
    }
   ],
   "source": [
    "vect_2 = CountVectorizer(stop_words=stop, ngram_range=(1, 2)).fit(X_train)\n",
    "X_train_vectorized_2 = vect_2.transform(X_train)\n",
    "\n",
    "clf2 = MultinomialNB()\n",
    "clf2.fit(X_train_vectorized_2, y_train) # ajusta al calificador 1\n",
    "\n",
    "X_test_vectorized_2 = vect_2.transform(X_test)\n",
    "predictions2 = clf2.predict(X_test_vectorized_2) # funcion para predecir 1\n",
    "\n",
    "print('Predicciones y resultados del clasificador 2')\n",
    "print('Predicciones', predictions2)\n",
    "# métricas para verificar que tanto es el calificador es correcto\n",
    "print( 'Accuracy', accuracy_score(y_test, predictions2) )\n",
    "print( 'AUC', roc_auc_score(y_test, predictions2) )\n",
    "print( 'F1', f1_score(y_test, predictions2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **3**. Ajuste un modelo de clasificación Naive Bayes multinomial.\n",
    "\n",
    "Transforme otro modelo de clasificación pero ajustando los datos de entrenamiento: eliminando las palabras vacías o cerradas.\n",
    "\n",
    "Devuelva la métrica F1 de evaluación de los modelos predictivos resultantes usando los datos de prueba transformados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de uso de  TfidfVectorizer(min_df=3)  [Modelo de formación de la matriz termino-documento ]+ MultinomialNB()  [Modelo de aprendizaje supervisado de clasificación]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador 1 - Métrica F1 0.8806818181818182\n"
     ]
    }
   ],
   "source": [
    "vect_tfid1 = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized_tfid1 = vect_tfid1.transform(X_train)\n",
    "\n",
    "clf1=MultinomialNB()\n",
    "clf1.fit(X_train_vectorized_tfid1, y_train) #ajusta al calificador 1\n",
    "\n",
    "X_test_vectorized_tfid1 = vect_tfid1.transform(X_test)\n",
    "\n",
    "predictions1_tfid1 = clf1.predict(X_test_vectorized_tfid1)\n",
    "print( 'Clasificador 1 - Métrica F1', f1_score(y_test, predictions1_tfid1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo de uso de  TfidfVectorizer(min_df=3, stop_words=stop) [Modelo de formación de la matriz termino-documento ]+ MultinomialNB()  [Modelo de aprendizaje supervisado de clasificación]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador 2 - Métrica F1 0.9146005509641872\n"
     ]
    }
   ],
   "source": [
    "vect_tfid2 = TfidfVectorizer(min_df=3, stop_words=stop).fit(X_train)\n",
    "X_train_vectorized_tfid2 = vect_tfid2.transform(X_train)\n",
    "\n",
    "clf2=MultinomialNB()\n",
    "clf2.fit(X_train_vectorized_tfid2, y_train) #ajusta al calificador 1\n",
    "\n",
    "X_test_vectorized_tfid2 = vect_tfid2.transform(X_test)\n",
    "\n",
    "predictions2_tfid2 = clf2.predict(X_test_vectorized_tfid2)\n",
    "print( 'Clasificador 2 - Métrica F1', f1_score(y_test, predictions2_tfid2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5**.\n",
    "\n",
    "Usando esta matriz de término de documento y una característica adicional, **la longitud del documento (número de caracteres)**, ajustar a un modelo de Clasificación de Vector de Soporte.\n",
    "\n",
    "*Esta función debe devolver el F1 como un flotante.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega como “caracteristica extra el numero de catacteres que tiene mis documentos + cantidad de digitos de documento lOGISTIC REGRESSION con regularización de 100”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 1468)\n",
      "(4179, 1469)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.29437229437229434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "num_chars = X_train.str.len()\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized = add_feature(X_train_vectorized, num_chars)\n",
    "print(X_train_vectorized.shape)\n",
    "\n",
    "model = SVC() # con regularización 'C = 10000'\n",
    "modelLin = LinearSVC()\n",
    "\n",
    "model.fit(X_train_vectorized, y_train)   \n",
    "modelLin.fit(X_train_vectorized, y_train)  \n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "num_charsTest = X_test.str.len()\n",
    "X_test_vectorized = add_feature(X_test_vectorized, num_charsTest)\n",
    "\n",
    "predictions = model.predict(X_test_vectorized)\n",
    "predictionsLin = modelLin.predict(X_test_vectorized)\n",
    "\n",
    "print(f1_score(y_test, predictions), f1_score(y_test, predictionsLin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y usando **n-grams de palabras n = 1 a n = 3** (unigramas, bigramas y trigramas).\n",
    "\n",
    "Usando esta matriz de término-documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* **cantidad de dígitos por documento**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`.\n",
    "\n",
    "*Esta función debe devolver el F1 como un flotante.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega como “caracteristica extra el numero de catacteres que tiene mis documentos + cantidad de digitos de documento lOGISTIC REGRESSION con regularización de 100”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9690721649484535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "   \n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "num_chars = X_train.str.len()\n",
    "num_digits = X_train.str.count('\\d')\n",
    "X_train_vectorized = add_feature(X_train_vectorized, num_chars)\n",
    "X_train_vectorized = add_feature(X_train_vectorized, num_digits)\n",
    "model = LogisticRegression(C=100)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "num_charsTest = X_test.str.len()\n",
    "num_digitsTest = X_test.str.count('\\d')\n",
    "X_test_vectorized = add_feature(X_test_vectorized, num_charsTest)\n",
    "X_test_vectorized = add_feature(X_test_vectorized, num_digitsTest)    \n",
    "\n",
    "predictions = model.predict(X_test_vectorized)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más ejercicios!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7\n",
    "\n",
    "Ajuste los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "¿Cuál es el token más largo en el vocabulario?\n",
    "\n",
    "*Esta función debería devolver una cadena.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def respuesta_siete():\n",
    "    vect = CountVectorizer().fit(X_train) # ajuste a partir del conjunto de entrenamiento\n",
    "    \n",
    "    ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_siete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 8\n",
    "\n",
    "¿Cuál es la longitud promedio de los documentos (número de caracteres) para los documentos spam y no spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (longitud promedio no spam, longitud promedio de spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_ocho():    \n",
    "    \n",
    "    ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_ocho()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 9\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_nueve():\n",
    "    \n",
    "    ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_nueve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 10\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_diez():\n",
    "    \n",
    "    ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_diez()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 11\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `CountVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y utilizando **caracteres n-grams desde n = 2 a n = 5.**\n",
    "\n",
    "Para decirle a `CountVectorizer` que use caracteres n-grams, pase en `analyzer = 'char_wb'` que crea caracteres n-gramas solo del texto dentro de los límites de las palabras. Esto debería hacer que el modelo sea más robusto a los errores ortográficos.\n",
    "\n",
    "Usando esta matriz término documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* cantidad de dígitos por documento\n",
    "* **cantidad de caracteres que no son palabras (cualquier cosa que no sea una letra, dígito o guión bajo).**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "También **encuentre los 10 coeficientes más pequeños y los 10 más grandes del modelo** y devuélvalos junto con el AUC en una tupla.\n",
    "\n",
    "La lista de los 10 coeficientes más pequeños debe ordenarse primero, la lista de los 10 coeficientes más grandes debe ordenarse primero.\n",
    "\n",
    "Las tres características que se agregaron a la matriz de términos del documento deben tener los siguientes nombres si aparecen en la lista de coeficientes:\n",
    "['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "\n",
    "*Esta función debe devolver una tupla `(AUC como flotante, lista de coeficientes más pequeños, lista de coeficientes más grande)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_once():    \n",
    "    \n",
    "    ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
