{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#03997A\"> Diplomado en Ciencia de datos UNAM </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Dr: Alejandro Pimentel  </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Modulo 8 Introducción al Deep Learning </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Tema 2 Backpropagation  Alumno: Ibarra Ramírez Sergio </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizaran los mismos datos de las compuertas lógicas. En donde se tuvo que actualizar el y_train a un modelo one -hot para usar la función de activación de softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de compuarta NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2023)\n",
    "\n",
    "def mostrar_pesos(w):\n",
    "    print('b =', '%.1f' % w[0],\n",
    "          'w1 =', '%.1f' % w[1],\n",
    "          'w2 =', '%.1f' % w[2]\n",
    "         )\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "lista_indices = [0,1,2,3]\n",
    "\n",
    "x_train = [\n",
    "    (1,-1,-1),\n",
    "    (1,-1,1),\n",
    "    (1,1,-1),\n",
    "    (1,1,1)\n",
    "]\n",
    "\n",
    "# Cambiamos la salida a one-hot\n",
    "y_train = [\n",
    "    (0,1),\n",
    "    (0,1),\n",
    "    (0,1),\n",
    "    (1,0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función para inicializar pesos aleatorios (esta funión puede ser usada para generar pesos aleatorios cada que se necesite implementar una nueva neurona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_pesos(num_entradas):\n",
    "  pesos = [0]\n",
    "  for i in range(num_entradas):\n",
    "    pesos.append(random.uniform(-1,1))\n",
    "  return pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la cala oculta y la capa softmax con dos neuronas cada una "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, -0.2341561510915824, 0.9437241769815645],\n",
       " [0, 0.6876348464076729, -0.35943872315862]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_w = [genera_pesos(2),genera_pesos(2)]\n",
    "\n",
    "l1_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función que muestra los pesos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_pesos(lista_w):\n",
    "    for i,W in enumerate(lista_w):\n",
    "        print(\"Neurona \", i, end=\": \")\n",
    "        for j,w in enumerate(W):\n",
    "            print(f'w{j} =', '%.2f' % w, end=\", \")\n",
    "        print()\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurona  0: w0 = 0.00, w1 = -0.23, w2 = 0.94, \n",
      "Neurona  1: w0 = 0.00, w1 = 0.69, w2 = -0.36, \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "mostrar_pesos(l1_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FORWARD-Pass para el càlculo de la salida de la neurona (en este caso la probabilidad del softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def forward_pass(l1_w,X):\n",
    "  l1_y = []\n",
    "  for W in l1_w:\n",
    "    y=0\n",
    "    for x,w in zip(X,W):\n",
    "      y += w*x\n",
    "\n",
    "    l1_y.append(y) #logits\n",
    "\n",
    "  # Softmax\n",
    "  l2_y = []\n",
    "  denominador = 0\n",
    "  for x in l1_y:\n",
    "    denominador += math.exp(x)\n",
    "  for l2_x in l1_y:\n",
    "    numerador = math.exp(l2_x)\n",
    "    l2_y.append(numerador/denominador)\n",
    "\n",
    "  return l2_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BACKPROPAGATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tendrà un gradiente para CADA saluda de cada neurona y entonces por cada salida se tendrá una actualización de los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Se inicializa el gradiente en cero \n",
    "Nota: El profe mencionó \"Al cross entropy solo le importa la probabilidad que se le asigna a la salida correcta\".\n",
    "En el paso si a== j, define esa igualdad \n",
    "Nota: Si es la clase correcta el gradiente se actualiza y si no no "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(y_true,fwd_result,X):\n",
    "  grads = [[0 for i in X] for j in fwd_result]\n",
    "  for a,valor in enumerate(y_true):\n",
    "    if valor: # Identifica la salida correcta\n",
    "      for j, prob in enumerate(fwd_result):\n",
    "        for i,x in enumerate(X):\n",
    "          if a == j:\n",
    "            grad = x*(prob-1)\n",
    "          else:\n",
    "            grad = prob*x\n",
    "          grads[j][i] += grad\n",
    "    else: # Si no se trata de la clase correcta  no se hace nada\n",
    "      pass\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se define la función de actualización de pesos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizacion(l1_w,grads,L):\n",
    "  nuevos_w = [[w for w in W] for W in l1_w] # nuevos_w = l1_w # si hago esto, las dos variables en realidad serían la misma\n",
    "  for j,W in enumerate(l1_w):\n",
    "    for i,w in enumerate(W):\n",
    "      nuevos_w[j][i] += - L * grads[j][i]\n",
    "  return nuevos_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso del algoritmo de forward y backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in lista_indices \n",
    "\n",
    "Se calcula la y estimada con respecto a los pesos y datos\n",
    "Se calcula el máximo de la y que es el array que se acaba de calcular \n",
    "\n",
    "Para la y real que la tengo en la y_train también se calculará el indice màximo \n",
    "\n",
    "Si el Y real es diferente al y estimado, entonces no es correcto \n",
    "\n",
    "Se calculan los gradientes y se actualizan los pesos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurona  0: w0 = 0.00, w1 = -0.23, w2 = 0.94, \n",
      "Neurona  1: w0 = 0.00, w1 = 0.69, w2 = -0.36, \n",
      "--------------------\n",
      "Neurona  0: w0 = -0.10, w1 = -0.07, w2 = 0.94, \n",
      "Neurona  1: w0 = 0.10, w1 = 0.53, w2 = -0.36, \n",
      "--------------------\n",
      "Neurona  0: w0 = -0.19, w1 = 0.07, w2 = 0.94, \n",
      "Neurona  1: w0 = 0.19, w1 = 0.39, w2 = -0.35, \n",
      "--------------------\n",
      "Neurona  0: w0 = -0.26, w1 = 0.19, w2 = 0.93, \n",
      "Neurona  1: w0 = 0.26, w1 = 0.26, w2 = -0.34, \n",
      "--------------------\n",
      "Neurona  0: w0 = -0.33, w1 = 0.29, w2 = 0.92, \n",
      "Neurona  1: w0 = 0.33, w1 = 0.16, w2 = -0.34, \n",
      "--------------------\n",
      "Neurona  0: w0 = -0.38, w1 = 0.39, w2 = 0.92, \n",
      "Neurona  1: w0 = 0.38, w1 = 0.07, w2 = -0.33, \n",
      "--------------------\n",
      "Neurona  0: w0 = -0.43, w1 = 0.46, w2 = 0.92, \n",
      "Neurona  1: w0 = 0.43, w1 = -0.01, w2 = -0.33, \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "iters = 0\n",
    "while True:\n",
    "  if iters > 1000: break # por si acaso\n",
    "  mostrar_pesos(l1_w)\n",
    "  iters+=1\n",
    "  correcto = True\n",
    "\n",
    "  random.shuffle(lista_indices)\n",
    "  for i in lista_indices:\n",
    "    # Revisar si el modelo ya esta bien\n",
    "    h_y = forward_pass(l1_w,x_train[i])\n",
    "    max_hy = h_y.index(max(h_y))\n",
    "    y = y_train[i]\n",
    "    max_y = y.index(max(y))\n",
    "    if max_y != max_hy:\n",
    "      correcto = False\n",
    "\n",
    "  if correcto: break\n",
    "\n",
    "  nuevos_w = [[w for w in W] for W in l1_w]\n",
    "\n",
    "  # Back Propagation\n",
    "  for i in lista_indices:\n",
    "    h_y = forward_pass(l1_w,x_train[i])\n",
    "    grads = backward_pass(y_train[i],h_y,x_train[i])\n",
    "    nuevos_w = actualizacion(nuevos_w,grads,lr)\n",
    "\n",
    "  l1_w = nuevos_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7044732439566229, 0.2955267560433771]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicción\n",
    "forward_pass(l1_w,(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, -1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
