{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#03997A\"> Diplomado en Ciencia de datos UNAM </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Dr: Alejandro Pimentel  </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Modulo 8 Introducción al Deep Learning </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Ejercicio 4 CNN  Alumno: Ibarra Ramírez Sergio </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "c:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X,y = fetch_openml('mnist_784', as_frame=False, return_X_y=True)\n",
    "\n",
    "X = normalize(X).reshape(-1,28,28,1)\n",
    "\n",
    "X_train = X[:60000]\n",
    "X_test = X[60000:]\n",
    "y_train = y[:60000]\n",
    "y_test = y[60000:]\n",
    "\n",
    "codificador = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "y_train = codificador.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = codificador.transform(y_test.reshape(-1,1))\n",
    "\n",
    "X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "X_test = tf.constant(X_test, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train, dtype=tf.float32)\n",
    "y_test = tf.constant(y_test, dtype=tf.float32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "batch_size = 100 # Hiperparámetro\n",
    "train_dataset = train_dataset.shuffle(batch_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (100, 10) and (3200, 10) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sergio\\Documents\\SIR_Personal_Dell\\Diplomado_DC_UNAM\\Diplomado_DC_UNAM_FULL_Dell\\Diplomado_UNAM_DC_Full\\Modulo8_Introduccion_Deep_Learning\\Mod8_Ejercicio4_CNN.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m   dense \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(flat, W) \u001b[39m+\u001b[39m b  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   dense \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu(dense)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mcategorical_crossentropy(y_batch, dense)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, [W, b, conv1_filter, conv1_bias, conv2_filter, conv2_bias])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, [W, b, conv1_filter, conv1_bias, conv2_filter, conv2_bias]))\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2122\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[0;32m   2114\u001b[0m     \u001b[39mreturn\u001b[39;00m y_true \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m label_smoothing) \u001b[39m+\u001b[39m (\n\u001b[0;32m   2115\u001b[0m         label_smoothing \u001b[39m/\u001b[39m num_classes\n\u001b[0;32m   2116\u001b[0m     )\n\u001b[0;32m   2118\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39msmart_cond\u001b[39m.\u001b[39msmart_cond(\n\u001b[0;32m   2119\u001b[0m     label_smoothing, _smooth_labels, \u001b[39mlambda\u001b[39;00m: y_true\n\u001b[0;32m   2120\u001b[0m )\n\u001b[1;32m-> 2122\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcategorical_crossentropy(\n\u001b[0;32m   2123\u001b[0m     y_true, y_pred, from_logits\u001b[39m=\u001b[39;49mfrom_logits, axis\u001b[39m=\u001b[39;49maxis\n\u001b[0;32m   2124\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:5560\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   5558\u001b[0m target \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(target)\n\u001b[0;32m   5559\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n\u001b[1;32m-> 5560\u001b[0m target\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49massert_is_compatible_with(output\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m   5562\u001b[0m output, from_logits \u001b[39m=\u001b[39m _get_logits(\n\u001b[0;32m   5563\u001b[0m     output, from_logits, \u001b[39m\"\u001b[39m\u001b[39mSoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5564\u001b[0m )\n\u001b[0;32m   5565\u001b[0m \u001b[39mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (100, 10) and (3200, 10) are incompatible"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "W = tf.Variable(tf.random.truncated_normal([1568, 10], stddev=0.1)) \n",
    "\n",
    "conv1_filter = tf.Variable(tf.random.truncated_normal([4, 4, 1, 32], stddev=0.1))\n",
    "conv1_bias =  tf.Variable(tf.random.truncated_normal([32], stddev=0.1))\n",
    "\n",
    "conv2_filter = tf.Variable(tf.random.truncated_normal([4, 4, 32, 64], stddev=0.1))\n",
    "conv2_bias = tf.Variable(tf.random.truncated_normal([64], stddev=0.1))\n",
    "\n",
    "# Training loop\n",
    "optimizer = tf.optimizers.Adam()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  for X_batch, y_batch in train_dataset:\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "      conv1 = tf.nn.conv2d(X_batch, conv1_filter, strides=1, padding='SAME')\n",
    "      conv1 = tf.nn.bias_add(conv1, conv1_bias)\n",
    "      conv1 = tf.nn.relu(conv1)\n",
    "      \n",
    "      conv2 = tf.nn.conv2d(conv1, conv2_filter, strides=1, padding='SAME')  \n",
    "      conv2 = tf.nn.bias_add(conv2, conv2_bias)\n",
    "      conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "      flat = tf.reshape(conv2, [-1, 1568])\n",
    "      \n",
    "      dense = tf.matmul(flat, W) + b  \n",
    "      dense = tf.nn.relu(dense)\n",
    "      \n",
    "      loss = tf.losses.categorical_crossentropy(y_batch, dense)\n",
    "\n",
    "    gradients = tape.gradient(loss, [W, b, conv1_filter, conv1_bias, conv2_filter, conv2_bias])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b, conv1_filter, conv1_bias, conv2_filter, conv2_bias]))\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Loss: {loss.numpy():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [100] vs. [10000] [Op:Equal] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sergio\\Documents\\SIR_Personal_Dell\\Diplomado_DC_UNAM\\Diplomado_DC_UNAM_FULL_Dell\\Diplomado_UNAM_DC_Full\\Modulo8_Introduccion_Deep_Learning\\Mod8_Ejercicio4_CNN.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m max_preds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39margmax(probs,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m max_trues \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39margmax(y_test,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m numCorrectos \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mequal(max_preds,max_trues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m acc \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mcast(numCorrectos,tf\u001b[39m.\u001b[39mfloat32)) \u001b[39m# Tengo que convertirlo porque el original es booleano\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(acc)\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [100] vs. [10000] [Op:Equal] name: "
     ]
    }
   ],
   "source": [
    "sasalida_conv1 = tf.nn.conv2d(X_test,filtros1,[1,2,2,1],\"SAME\")\n",
    "salida_conv1 += b_conv1\n",
    "salida_activacion1 = tf.nn.relu(salida_conv1) # Capa no lineal intermedia\n",
    "salida_conv2 = tf.nn.conv2d(salida_activacion1,filtros2,[1,2,2,1],\"SAME\")\n",
    "salida_conv2 += b_conv2\n",
    "salida_activacion2 =tf.nn.relu(salida_conv2) # necesita el relu?\n",
    "\n",
    "salidaPlana = tf.reshape(salida_conv2,[-1,1568])\n",
    "\n",
    "operacion_matricial = tf.matmul(salidaPlana,W)+b # Primera capa\n",
    "\n",
    "probs = tf.nn.softmax(operacion_matricial)\n",
    "\n",
    "max_preds = tf.argmax(probs,1)\n",
    "max_trues = tf.argmax(y_test,1)\n",
    "\n",
    "numCorrectos = tf.equal(max_preds,max_trues)\n",
    "acc = tf.reduce_mean(tf.cast(numCorrectos,tf.float32)) # Tengo que convertirlo porque el original es booleano\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
