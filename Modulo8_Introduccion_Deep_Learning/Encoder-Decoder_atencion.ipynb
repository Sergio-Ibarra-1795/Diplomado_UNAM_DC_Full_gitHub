{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#03997A\"> Diplomado en Ciencia de datos UNAM </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Dr: Alejandro Pimentel  </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Modulo 8 Introducción al Deep Learning </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Tema 5.1 Encoder-Decoer & ATENCIÓN Alumno: Ibarra Ramírez Sergio </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_oracion = 300\n",
    "num_palabras = 5000\n",
    "emb_dim = 200\n",
    "\n",
    "#### De que tamanio va ser el vector con que se representarña la oración total\n",
    "h_dim = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras vectorizadas, vana ser las entradas, pasadas por un embeding (Es como se hacía en el secuencial en donde se definnia como primera capa un embeing donde se especificaba su tamanio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 200) dtype=float32 (created by layer 'embedding')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas_codificador = tf.keras.layers.Input(shape=(None,))\n",
    "embeddings = tf.keras.layers.Embedding(num_palabras,emb_dim)\n",
    "palabras_vectorizadas = embeddings(entradas_codificador)\n",
    "palabras_vectorizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va ausar la arquitectura GRU como encodficiador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 600) dtype=float32 (created by layer 'gru')>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codificador_gru = tf.keras.layers.GRU(h_dim,recurrent_dropout=0.5,return_state=True)\n",
    "salida_cod, estado_cod = codificador_gru(palabras_vectorizadas)\n",
    "salida_cod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x2b9199336a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_codificador = tf.keras.models.Model(entradas_codificador,estado_cod)\n",
    "modelo_codificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las entradas del decodificador será obviamente las salidas del codificador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 600) dtype=float32 (created by layer 'input_2')>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas_decodificador = tf.keras.layers.Input(shape=salida_cod.shape)\n",
    "entradas_decodificador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po tro lado return_sequences retorna una matriz como secuencia de palabras de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.rnn.gru.GRU at 0x2b90ae89840>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decodificador_gru = tf.keras.layers.GRU(h_dim,recurrent_dropout=0.5, return_sequences=True)\n",
    "decodificador_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 600) dtype=float32 (created by layer 'gru_1')>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entrada_estado_decod = tf.keras.layers.Input(shape=(h_dim,))\n",
    "salidas_decod = decodificador_gru(entradas_decodificador,initial_state=entrada_estado_decod)\n",
    "salidas_decod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x2b919a6ea40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decodificador_densa = tf.keras.layers.Dense(num_palabras, activation='softmax')\n",
    "decodificador_densa \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la ultima capa despues de las primeras dos RNN (En este caso GRU) será una Dense de tipo SOFTMAX, en donde la salida de la GRU Decoder se le pasa al a densa de SOFTMAX para predecir cada caso la palabras más probable siguiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x2b919a6e620>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salidas_decod = decodificador_densa(salidas_decod)\n",
    "modelo_decodificador = tf.keras.models.Model([entrada_estado_decod,entradas_decodificador],salidas_decod)\n",
    "modelo_decodificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x2bc0aaa5570>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificador\n",
    "entradas_codificador = tf.keras.layers.Input(shape=(None,))\n",
    "embeddings = tf.keras.layers.Embedding(num_palabras,emb_dim)\n",
    "palabras_vectorizadas = embeddings(entradas_codificador)\n",
    "\n",
    "codificador_gru = tf.keras.layers.GRU(h_dim,recurrent_dropout=0.5,return_state=True,return_sequences=True)\n",
    "salidas_cod, estado_cod = codificador_gru(palabras_vectorizadas)\n",
    "\n",
    "modelo_codificador =  tf.keras.models.Model(entradas_codificador,estado_cod)\n",
    "modelo_codificador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificador\n",
    "entradas_decodificador = tf.keras.layers.Input(shape=salidas_cod.shape)\n",
    "decodificador_gru = tf.keras.layers.GRU(h_dim,recurrent_dropout=0.5, return_sequences=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, None, 600) dtype=float32 (created by layer 'input_10')>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas_decodificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entrada_estado_decod = tf.keras.layers.Input(shape=(h_dim,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"gru_5\" (type GRU).\n\nShapes must be equal rank, but are 3 and 2\n\nCall arguments received by layer \"gru_5\" (type GRU):\n  • inputs=['tf.Tensor(shape=(None, None, None, 600), dtype=float32)', 'tf.Tensor(shape=(None, 600), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sergio\\Documents\\SIR_Personal_Dell\\Diplomado_DC_UNAM\\Diplomado_DC_UNAM_FULL_Dell\\Diplomado_UNAM_DC_Full\\Modulo8_Introduccion_Deep_Learning\\Encoder-Decoder_atencion.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Encoder-Decoder_atencion.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m salidas_decod \u001b[39m=\u001b[39m decodificador_gru(entradas_decodificador,initial_state\u001b[39m=\u001b[39;49mentrada_estado_decod)\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\base_rnn.py:615\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[39m# Perform the call with temporarily replaced input_spec\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m full_input_spec\n\u001b[1;32m--> 615\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(full_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    616\u001b[0m \u001b[39m# Remove the additional_specs from input spec and keep the rest. It\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39m# is important to keep since the input spec was populated by\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m# build(), and will be reused in the stateful=True.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(additional_specs)]\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:5156\u001b[0m, in \u001b[0;36mrnn.<locals>._step\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m   5154\u001b[0m \u001b[39mfor\u001b[39;00m state, new_state \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_state, flat_new_state):\n\u001b[0;32m   5155\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(new_state, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m-> 5156\u001b[0m         new_state\u001b[39m.\u001b[39;49mset_shape(state\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m   5158\u001b[0m flat_output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(output)\n\u001b[0;32m   5159\u001b[0m ta_index_to_write \u001b[39m=\u001b[39m time \u001b[39mif\u001b[39;00m return_all_outputs \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"gru_5\" (type GRU).\n\nShapes must be equal rank, but are 3 and 2\n\nCall arguments received by layer \"gru_5\" (type GRU):\n  • inputs=['tf.Tensor(shape=(None, None, None, 600), dtype=float32)', 'tf.Tensor(shape=(None, 600), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "salidas_decod = decodificador_gru(entradas_decodificador,initial_state=entrada_estado_decod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la atención se le debe definir \"que cosa le va a poner atencióna a que cosa\" y entonces se dice que el estado o salida del decodificador le tiene que poner atención a las saalidas del enconder que son las entradas del decoder (a todo el historial)\n",
    "Y eso nos va a dar el vector de contexto que es el vector ponderado del que se habló hace un momento (el azulito)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces se concatenan los contextos con la salida del GRU \"ORIGINAL\" y ese va a fungir como el nuevo estado con la que se va a llevar a cabo la clasificación del SOFTMAX, que en lugar de recibir unicamente la salida del decodificador, recibirña clos contextos a los que se le est´´a poniendo atención en ese momento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "atencion = tf.keras.layers.Attention()\n",
    "contextos = atencion([salidas_decod,entradas_decodificador])\n",
    "\n",
    "resultado_atencion = tf.concat([contextos, salidas_decod], axis=-1)\n",
    "\n",
    "decodificador_densa = tf.keras.layers.Dense(num_palabras, activation='softmax')\n",
    "\n",
    "salidas_decod = decodificador_densa(resultado_atencion)\n",
    "\n",
    "modelo_decodificador = tf.keras.models.Model([entrada_estado_decod,entradas_decodificador],salidas_decod)\n",
    "modelo_decodificador \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atencion en un solo paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 600) dtype=float32 (created by layer 'attention')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificador\n",
    "entradas_codificador = tf.keras.layers.Input(shape=(None,))\n",
    "embeddings = tf.keras.layers.Embedding(num_palabras,emb_dim)\n",
    "palabras_vectorizadas = embeddings(entradas_codificador)\n",
    "\n",
    "codificador_gru = tf.keras.layers.GRU(h_dim,recurrent_dropout=0.5,return_state=True,return_sequences=True)\n",
    "salidas_cod, estado_cod = codificador_gru(palabras_vectorizadas)\n",
    "\n",
    "# Decodificador\n",
    "\n",
    "decodificador_gru = tf.keras.layers.GRU(h_dim,recurrent_dropout=0.5, return_sequences=True)\n",
    "\n",
    "salidas_decod = decodificador_gru(salidas_cod,initial_state=estado_cod)\n",
    "\n",
    "atencion = tf.keras.layers.Attention()\n",
    "contextos = atencion([salidas_decod,salidas_cod])\n",
    "contextos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
