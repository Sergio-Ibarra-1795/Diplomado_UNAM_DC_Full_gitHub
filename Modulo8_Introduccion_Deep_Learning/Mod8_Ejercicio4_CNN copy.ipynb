{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#03997A\"> Diplomado en Ciencia de datos UNAM </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Dr: Alejandro Pimentel  </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Modulo 8 Introducción al Deep Learning </font>\n",
    "\n",
    "##### <font color=\"#03997A\"> Ejercicio 4 CNN  Alumno: Ibarra Ramírez Sergio </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sergio\\Documents\\SIR_Personal_Dell\\Diplomado_DC_UNAM\\Diplomado_DC_UNAM_FULL_Dell\\Diplomado_UNAM_DC_Full\\Modulo8_Introduccion_Deep_Learning\\Mod8_Ejercicio4_CNN copy.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN%20copy.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_train, X_test \u001b[39m=\u001b[39m X[:\u001b[39m60000\u001b[39m], X[\u001b[39m60000\u001b[39m:] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN%20copy.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y_train, y_test \u001b[39m=\u001b[39m y[:\u001b[39m60000\u001b[39m], y[\u001b[39m60000\u001b[39m:]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN%20copy.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_train \u001b[39m=\u001b[39m OneHotEncoder()\u001b[39m.\u001b[39mfit_transform(y_train\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN%20copy.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_test \u001b[39m=\u001b[39m OneHotEncoder()\u001b[39m.\u001b[39mtransform(y_test\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergio/Documents/SIR_Personal_Dell/Diplomado_DC_UNAM/Diplomado_DC_UNAM_FULL_Dell/Diplomado_UNAM_DC_Full/Modulo8_Introduccion_Deep_Learning/Mod8_Ejercicio4_CNN%20copy.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Create dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# Load data\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True)\n",
    "X = normalize(X).reshape(-1, 28, 28, 1)\n",
    "\n",
    "X_train, X_test = X[:60000], X[60000:] \n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "y_train = OneHotEncoder().fit_transform(y_train.reshape(-1,1)).astype(float)\n",
    "y_test = OneHotEncoder().transform(y_test.reshape(-1,1)).astype(float)\n",
    "\n",
    "# Create dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(100) \n",
    "\n",
    "# Initialize weights\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "W = tf.Variable(tf.random.truncated_normal([1568, 10], stddev=0.1))\n",
    "\n",
    "conv1_f = tf.Variable(tf.random.truncated_normal([4, 4, 1, 32], stddev=0.1))\n",
    "conv1_b = tf.Variable(tf.random.truncated_normal([32], stddev=0.1)) \n",
    "\n",
    "conv2_f = tf.Variable(tf.random.truncated_normal([4, 4, 32, 64], stddev=0.1))\n",
    "conv2_b = tf.Variable(tf.random.truncated_normal([64], stddev=0.1))\n",
    "\n",
    "# Training loop\n",
    "optimizer = tf.optimizers.Adam()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  for X_batch, y_batch in train_ds:\n",
    "\n",
    "    with tf.GradientTape() as tape: \n",
    "    \n",
    "      conv1 = tf.nn.conv2d(X_batch, conv1_f, strides=1, padding='SAME')  \n",
    "      conv1 = tf.nn.bias_add(conv1, conv1_b)\n",
    "      conv1 = tf.nn.relu(conv1)   \n",
    "\n",
    "      conv2 = tf.nn.conv2d(conv1, conv2_f, strides=1, padding='SAME')\n",
    "      conv2 = tf.nn.bias_add(conv2, conv2_b)\n",
    "      conv2 = tf.nn.relu(conv2)\n",
    "      \n",
    "      # Flatten conv2 output preserving batch size\n",
    "      conv2_shape = conv2.get_shape().as_list()\n",
    "      flat = tf.reshape(conv2, [-1, conv2_shape[1] * conv2_shape[2] * conv2_shape[3]]) \n",
    "\n",
    "      dense = tf.matmul(flat, W) + b\n",
    "      dense = tf.nn.relu(dense)\n",
    "\n",
    "      loss = tf.losses.categorical_crossentropy(y_batch, dense)\n",
    "\n",
    "    gradients = tape.gradient(loss, [W, b, conv1_f, conv1_b, conv2_f, conv2_b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b, conv1_f, conv1_b, conv2_f, conv2_b]))\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Loss: {loss.numpy():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
